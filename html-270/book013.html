<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="hevea 2.09" />
<link rel="stylesheet" type="text/css" href="book.css" />
<title>Networked programs</title>
</head>
<body>
<a href="book012.html"><img src="previous_motif.gif" alt="Previous" /></a>
<a href="index.html"><img src="contents_motif.gif" alt="Up" /></a>
<a href="book014.html"><img src="next_motif.gif" alt="Next" /></a>
<hr />
<h1 class="chapter" id="sec147"><span class="c006">Chapter&#XA0;12&#XA0;&#XA0;Networked programs</span></h1>
<p><span class="c006">While many of the examples in this book have focused on reading
files and looking for data in those files, there are many different
sources of information when one considers the Internet.</span></p><p><span class="c006">In this chapter we will pretend to be a web browser and retrieve web
pages using the HyperText Transport Protocol (HTTP). Then we will read
through the web page data and parse it.</span></p><span class="c005">
</span><h2 class="section" id="sec148"><span class="c006">12.1&#XA0;&#XA0;HyperText Transport Protocol - HTTP</span></h2>
<p><span class="c006">The network protocol that powers the web is actually quite simple and 
there is built-in support in Python called <span class="c001">sockets</span> which makes it very 
easy to make network connections and retrieve data over those
sockets in a Python program.</span></p><p><span class="c006">A <span class="c009">socket</span> is much like a file, except that a single socket 
provides a two-way connection between two programs. 
You can both read from and write to the same socket. If you write something to 
a socket, it is sent to the application at the other end of the socket. If you 
read from the socket, you are given the data which the other application has sent.</span></p><p><span class="c006">But if you try to read a socket when the program on the other end of the socket
has not sent any data&#X2014;you just sit and wait. If the programs on both ends
of the socket simply wait for some data without sending anything, they will wait for
a very long time.</span></p><p><span class="c006">So an important part of programs that communicate over the Internet is to have some
sort of protocol. A protocol is a set of precise rules that determine who
is to go first, what they are to do, and then what the responses are to that message,
and who sends next, and so on. In a sense the two applications at either end 
of the socket are doing a dance and making sure not to step on each other&#X2019;s toes.</span></p><p><span class="c006">There are many documents which describe these network protocols. The HyperText Transport 
Protocol is described in the following document:</span></p><p><span class="c002">http://www.w3.org/Protocols/rfc2616/rfc2616.txt</span></p><p><span class="c006">This is a long and complex 176-page document with a lot of detail. If you 
find it interesting, feel free to read it all. But if you take a look around page 36 of
RFC2616 you will find the syntax for the GET request. To request a document from a web
server, we make a connection to the <span class="c001">data.py4e.org</span> server on port 80, and then
send a line of the form</span></p><p><span class="c002">GET http://data.py4e.org/romeo.txt HTTP/1.0 </span></p><p><span class="c006">where the second parameter is the web page we are requesting, and then 
we also send a blank line. The web server will respond with some 
header information about the document and a blank line
followed by the document content.</span></p><span class="c005">
</span><h2 class="section" id="sec149"><span class="c006">12.2&#XA0;&#XA0;The World&#X2019;s Simplest Web Browser</span></h2>
<p><span class="c006">Perhaps the easiest way to show how the HTTP protocol works is to write a very 
simple Python program that makes a connection to a web server and follows
the rules of the HTTP protocol to requests a document 
and display what the server sends back.</span></p><pre class="verbatim"><span class="c004">import socket

mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
mysock.connect(('data.py4e.org', 80))
mysock.send('GET http://data.py4e.org/romeo.txt HTTP/1.0\n\n')

while True:
    data = mysock.recv(512)
    if ( len(data) &lt; 1 ) :
        break
    print data

mysock.close()
</span></pre><p><span class="c006">First the program makes a connection to port 80 on 
the server <span class="c001">www.py4inf.com</span>.
Since our program is playing the role of the &#X201C;web browser&#X201D;, the HTTP
protocol says we must send the GET command followed by a blank line.</span></p><div class="center"><span class="c006"><img src="book013.png" /></span></div><p><span class="c006">Once we send that blank line, we write a loop that receives data 
in 512-character chunks from the socket and prints the data out 
until there is no more data to read (i.e., the recv() returns 
an empty string).</span></p><p><span class="c006">The program produces the following output:</span></p><pre class="verbatim"><span class="c004">HTTP/1.1 200 OK
Date: Sun, 14 Mar 2010 23:52:41 GMT
Server: Apache
Last-Modified: Tue, 29 Dec 2009 01:31:22 GMT
ETag: "143c1b33-a7-4b395bea"
Accept-Ranges: bytes
Content-Length: 167
Connection: close
Content-Type: text/plain

But soft what light through yonder window breaks
It is the east and Juliet is the sun
Arise fair sun and kill the envious moon
Who is already sick and pale with grief
</span></pre><p><span class="c006">The output starts with headers which the web server sends
to describe the document.
For example, the <span class="c001">Content-Type</span> header indicates that
the document is a plain text document (<span class="c001">text/plain</span>).</span></p><p><span class="c006">After the server sends us the headers, it adds a blank line
to indicate the end of the headers, and then sends the actual
data of the file <span class="c001">romeo.txt</span>.</span></p><p><span class="c006">This example shows how to make a low-level network connection
with sockets. Sockets can be used to communicate with a web
server or with a mail server or many other kinds of servers.
All that is needed is to find the document which describes
the protocol and write the code to send and receive the data
according to the protocol.</span></p><p><span class="c006">However, since the protocol that we use most commonly is
the HTTP web protocol, Python has a special 
library specifically designed to support the HTTP protocol 
for the retrieval of documents and data over the web.</span></p><span class="c005">
</span><h2 class="section" id="sec150"><span class="c006">12.3&#XA0;&#XA0;Retrieving an image over HTTP</span></h2>
<p><a id="hevea_default741"></a><span class="c005">
</span><a id="hevea_default742"></a><span class="c005">
</span><a id="hevea_default743"></a><span class="c006">
In the above example, we retrieved a plain text file 
which had newlines in the file and we simply copied the
data to the screen as the program ran. We can use a similar
program to retrieve an image across using HTTP. Instead
of copying the data to the screen as the program runs,
we accumulate the data in a string, trim off the headers,
and then save the image data to a file as follows:</span></p><pre class="verbatim"><span class="c004">import socket
import time

mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
mysock.connect(('www.py4inf.com', 80))
mysock.send('GET http://www.py4inf.com/cover.jpg HTTP/1.0\n\n')


count = 0
picture = "";
while True:
    data = mysock.recv(5120)
    if ( len(data) &lt; 1 ) : break
    # time.sleep(0.25)
    count = count + len(data)
    print len(data),count
    picture = picture + data

mysock.close()

# Look for the end of the header (2 CRLF)
pos = picture.find("\r\n\r\n");
print 'Header length',pos
print picture[:pos]

# Skip past the header and save the picture data
picture = picture[pos+4:]
fhand = open("stuff.jpg","wb")
fhand.write(picture);
fhand.close()
</span></pre><p><span class="c006">When the program runs it produces the following output:</span></p><pre class="verbatim"><span class="c004">$ python urljpeg.py 
2920 2920
1460 4380
1460 5840
1460 7300
...
1460 62780
1460 64240
2920 67160
1460 68620
1681 70301
Header length 240
HTTP/1.1 200 OK
Date: Sat, 02 Nov 2013 02:15:07 GMT
Server: Apache
Last-Modified: Sat, 02 Nov 2013 02:01:26 GMT
ETag: "19c141-111a9-4ea280f8354b8"
Accept-Ranges: bytes
Content-Length: 70057
Connection: close
Content-Type: image/jpeg
</span></pre><p><span class="c006">You can see that for this url, the 
<span class="c001">Content-Type</span> header indicates that
body of the document is an image (<span class="c001">image/jpeg</span>).
Once the program completes, you can view the image data by opening
the file <span class="c001">stuff.jpg</span> in an image viewer.</span></p><p><span class="c006">As the program runs, you can see that we don&#X2019;t get 5120 characters
each time we call the <span class="c001">recv()</span> method.
We get as many characters as have been transferred across the network 
to us by the web server at the moment we call <span class="c001">recv()</span>. 
In this example, we either get 1460 or 2920 characters each time we
request up to 5120 characters of data.</span></p><p><span class="c006">Your results may be different depending on your network speed. Also
note that on the last call to <span class="c001">recv()</span> we get 1681 bytes, which is the end
of the stream, and in the next call to <span class="c001">recv()</span> we get a zero-length
string that tells us that the server has called <span class="c001">close()</span> on its end 
of the socket and there is no more data forthcoming.</span></p><p><a id="hevea_default744"></a><span class="c005">
</span><a id="hevea_default745"></a><span class="c006">
We can slow down our successive <span class="c001">recv()</span> calls by uncommenting the call 
to <span class="c001">time.sleep()</span>. This way, we wait a quarter of a second after each call
so that the server can &#X201C;get ahead&#X201D; of us and send more data to us
before we call <span class="c001">recv()</span> again. With the delay, in place the program 
executes as follows:
</span></p><pre class="verbatim"><span class="c004">$ python urljpeg.py 
1460 1460
5120 6580
5120 11700
...
5120 62900
5120 68020
2281 70301
Header length 240
HTTP/1.1 200 OK
Date: Sat, 02 Nov 2013 02:22:04 GMT
Server: Apache
Last-Modified: Sat, 02 Nov 2013 02:01:26 GMT
ETag: "19c141-111a9-4ea280f8354b8"
Accept-Ranges: bytes
Content-Length: 70057
Connection: close
Content-Type: image/jpeg
</span></pre><p><span class="c006">Now other than the first and last calls to <span class="c001">recv()</span>, we now get 
5120 characters each time we ask for new data. </span></p><p><span class="c006">There is a buffer between the server making <span class="c001">send()</span> requests 
and our application making <span class="c001">recv()</span> requests. When we run the 
program with the delay in place, at some point the server might 
fill up the buffer in the socket and be forced to pause until our
program starts to empty the buffer. The pausing of either the 
sending application or the receiving application is called 
&#X201C;flow control&#X201D;.
</span><a id="hevea_default746"></a></p><span class="c005">
</span><h2 class="section" id="sec151"><span class="c006">12.4&#XA0;&#XA0;Retrieving web pages with <span class="c001">urllib</span></span></h2>
<p><span class="c006">While we can manually send and receive data over HTTP 
using the socket library, there is a much simpler way to 
perform this common task in Python by 
using the <span class="c001">urllib</span> library.</span></p><p><span class="c006">Using <span class="c001">urllib</span>,
you can treat a web page much like a file. You simply
indicate which web page you would like to retrieve and
<span class="c001">urllib</span> handles all of the HTTP protocol and header 
details.</span></p><p><span class="c006">The equivalent code to read the <span class="c001">romeo.txt</span> file
from the web using <span class="c001">urllib</span> is as follows:</span></p><pre class="verbatim"><span class="c004">import urllib

fhand = urllib.urlopen('http://www.py4inf.com/code/romeo.txt')
for line in fhand:
   print line.strip()
</span></pre><p><span class="c006">Once the web page has been opened with 
<span class="c001">urllib.urlopen</span>, we can treat it like 
a file and read through it using a 
<span class="c001">for</span> loop. </span></p><p><span class="c006">When the program runs, we only see the output
of the contents of the file. The headers
are still sent, but the <span class="c001">urllib</span> code
consumes the headers and only returns the 
data to us.</span></p><pre class="verbatim"><span class="c004">But soft what light through yonder window breaks
It is the east and Juliet is the sun
Arise fair sun and kill the envious moon
Who is already sick and pale with grief
</span></pre><p><span class="c006">As an example, we can write 
a program to retrieve the data for
<span class="c001">romeo.txt</span> and compute the frequency
of each word in the file as follows:</span></p><pre class="verbatim"><span class="c004">import urllib

counts = dict()
fhand = urllib.urlopen('http://www.py4inf.com/code/romeo.txt')
for line in fhand:
    words = line.split()
    for word in words:
        counts[word] = counts.get(word,0) + 1   
print counts
</span></pre><p><span class="c006">Again, once we have opened the web page, 
we can read it like a local file.</span></p><span class="c005">
</span><h2 class="section" id="sec152"><span class="c006">12.5&#XA0;&#XA0;Parsing HTML and scraping the web</span></h2>
<p><span class="c005">
</span><a id="hevea_default747"></a><span class="c005">
</span><a id="hevea_default748"></a></p><p><span class="c006">One of the common uses of the <span class="c001">urllib</span> capability in Python is 
to <span class="c009">scrape</span> the web. Web scraping is when we write a program
that pretends to be a web browser and retrieves pages, then 
examines the data in those pages looking for patterns.</span></p><p><span class="c006">As an example, a search engine such as Google will look at the source 
of one web page and extract the links to other pages and retrieve
those pages, extracting links, and so on. Using this technique,
Google <span class="c009">spiders</span> its way through nearly all of the pages on 
the web. </span></p><p><span class="c006">Google also uses the frequency of links from pages it finds 
to a particular page as one measure of how &#X201C;important&#X201D; 
a page is and how high the page should appear in its search results.</span></p><span class="c005">
</span><h2 class="section" id="sec153"><span class="c006">12.6&#XA0;&#XA0;Parsing HTML using regular expressions</span></h2>
<p><span class="c006">One simple way to parse HTML is to use regular expressions to repeatedly
search for and extract substrings that match a particular pattern.</span></p><p><span class="c006">Here is a simple web page:</span></p><pre class="verbatim"><span class="c004">&lt;h1&gt;The First Page&lt;/h1&gt;
&lt;p&gt;
If you like, you can switch to the
&lt;a href="http://www.dr-chuck.com/page2.htm"&gt;
Second Page&lt;/a&gt;.
&lt;/p&gt;
</span></pre><p><span class="c006">We can construct a well-formed regular expression to match
and extract the link values from the above text as follows:</span></p><pre class="verbatim"><span class="c004">href="http://.+?"
</span></pre><p><span class="c006">Our regular expression looks for strings that start with
&#X201C;href="http://&#X201D;, followed by one or more characters
(&#X201C;.+?&#X201D;), followed by another double quote. The question mark 
added to the &#X201C;.+?&#X201D; indicates that the match is to be done
in a &#X201C;non-greedy&#X201D; fashion instead of a &#X201C;greedy&#X201D; fashion. 
A non-greedy match tries to find the <em>smallest</em> possible matching
string and a greedy match tries to find the <em>largest</em> possible
matching string.
</span><a id="hevea_default749"></a><span class="c005">
</span><a id="hevea_default750"></a></p><p><span class="c006">We add parentheses to our regular expression to indicate
which part of our matched string we would like to extract, and
produce the following program:
</span><a id="hevea_default751"></a><span class="c005">
</span><a id="hevea_default752"></a></p><pre class="verbatim"><span class="c004">import urllib
import re

url = raw_input('Enter - ')
html = urllib.urlopen(url).read()
links = re.findall('href="(http://.*?)"', html)
for link in links:
    print link
</span></pre><p><span class="c006">The <span class="c001">findall</span> regular expression method will give us a list of all
of the strings that match our regular expression, returning only
the link text between the double quotes.</span></p><p><span class="c006">When we run the program, we get the following output:</span></p><pre class="verbatim"><span class="c004">python urlregex.py 
Enter - http://www.dr-chuck.com/page1.htm
http://www.dr-chuck.com/page2.htm

python urlregex.py 
Enter - http://www.py4inf.com/book.htm
http://www.greenteapress.com/thinkpython/thinkpython.html
http://allendowney.com/
http://www.py4inf.com/code
http://www.lib.umich.edu/espresso-book-machine
http://www.py4inf.com/py4inf-slides.zip
</span></pre><p><span class="c006">Regular expressions work very nicely when your HTML is well formatted
and predictable. But since there are a lot of &#X201C;broken&#X201D; HTML pages
out there, a solution only using regular expressions might either miss
some valid links or end up with bad data.</span></p><p><span class="c006">This can be solved by using a robust HTML parsing library.</span></p><span class="c005">
</span><h2 class="section" id="sec154"><span class="c006">12.7&#XA0;&#XA0;Parsing HTML using BeautifulSoup</span></h2>
<p><span class="c005">
</span><a id="hevea_default753"></a></p><p><span class="c006">There are a number of Python libraries which can help you parse
HTML and extract data from the pages. Each of the libraries
has its strengths and weaknesses and you can pick one based on 
your needs.</span></p><p><span class="c006">As an example, we will simply parse some HTML input 
and extract links using the <span class="c009">BeautifulSoup</span> library. 
You can download and install the BeautifulSoup code
from:</span></p><p><span class="c002">http://www.crummy.com/software/</span></p><p><span class="c006">You can download and &#X201C;install&#X201D; BeautifulSoup or you 
can simply place the <span class="c001">BeautifulSoup.py</span> file in the
same folder as your application.</span></p><p><span class="c006">Even though HTML looks like XML</span><sup><a id="text14" href="#note14"><span class="c006">1</span></a></sup><span class="c006"> and some pages are carefully 
constructed to be XML, most HTML is generally broken in ways
that cause an XML parser to reject the entire page of HTML as
improperly formed. BeautifulSoup tolerates highly flawed 
HTML and still lets you easily extract the data you need.</span></p><p><span class="c006">We will use <span class="c001">urllib</span> to read the page and then use
<span class="c001">BeautifulSoup</span> to extract the <span class="c001">href</span> attributes from the
anchor (<span class="c001">a</span>) tags.
</span><a id="hevea_default754"></a><span class="c005">
</span><a id="hevea_default755"></a><span class="c005">
</span><a id="hevea_default756"></a></p><pre class="verbatim"><span class="c004">import urllib
from BeautifulSoup import *

url = raw_input('Enter - ')
html = urllib.urlopen(url).read()
soup = BeautifulSoup(html)

# Retrieve all of the anchor tags
tags = soup('a')
for tag in tags:
   print tag.get('href', None)
</span></pre><p><span class="c006">The program prompts for a web address, then opens the web
page, reads the data and passes the data to the BeautifulSoup
parser, and then retrieves all of the anchor tags and prints
out the <span class="c001">href</span> attribute for each tag.</span></p><p><span class="c006">When the program runs it looks as follows:</span></p><pre class="verbatim"><span class="c004">python urllinks.py 
Enter - http://www.dr-chuck.com/page1.htm
http://www.dr-chuck.com/page2.htm

python urllinks.py 
Enter - http://www.py4inf.com/book.htm
http://www.greenteapress.com/thinkpython/thinkpython.html
http://allendowney.com/
http://www.si502.com/
http://www.lib.umich.edu/espresso-book-machine
http://www.py4inf.com/code
http://www.pythonlearn.com/
</span></pre><p><span class="c006">You can use BeautifulSoup to pull out various parts of each 
tag as follows:</span></p><pre class="verbatim"><span class="c004">import urllib
from BeautifulSoup import *

url = raw_input('Enter - ')
html = urllib.urlopen(url).read()
soup = BeautifulSoup(html)

# Retrieve all of the anchor tags
tags = soup('a')
for tag in tags:
   # Look at the parts of a tag
   print 'TAG:',tag
   print 'URL:',tag.get('href', None)
   print 'Content:',tag.contents[0]
   print 'Attrs:',tag.attrs
</span></pre><p><span class="c006">This produces the following output:</span></p><pre class="verbatim"><span class="c004">python urllink2.py 
Enter - http://www.dr-chuck.com/page1.htm
TAG: &lt;a href="http://www.dr-chuck.com/page2.htm"&gt;
Second Page&lt;/a&gt;
URL: http://www.dr-chuck.com/page2.htm
Content: [u'\nSecond Page']
Attrs: [(u'href', u'http://www.dr-chuck.com/page2.htm')]
</span></pre><p><span class="c006">These examples only begin to show the power of BeautifulSoup
when it comes to parsing HTML. See the documentation 
and samples at
<span class="c001">http://www.crummy.com/software/BeautifulSoup/</span> for more detail.</span></p><span class="c005">
</span><h2 class="section" id="sec155"><span class="c006">12.8&#XA0;&#XA0;Reading binary files using urllib</span></h2>
<p><span class="c006">Sometimes you want to retrieve a non-text (or binary) file such as
an image or video file. The data in these files is generally not 
useful to print out, but you can easily make a copy of a URL to a local
file on your hard disk using <span class="c001">urllib</span>.
</span><a id="hevea_default757"></a></p><p><span class="c006">The pattern is to open the URL and use <span class="c001">read</span> to download the entire
contents of the document into a string variable (<span class="c001">img</span>) then write that
information to a local file as follows:</span></p><pre class="verbatim"><span class="c004">img = urllib.urlopen('http://www.py4inf.com/cover.jpg').read()
fhand = open('cover.jpg', 'w')
fhand.write(img)
fhand.close()
</span></pre><p><span class="c006">This program reads all of the data in at once across the network and 
stores it in the variable <span class="c001">img</span> in the main memory of your computer,
then opens the file <span class="c001">cover.jpg</span> and writes the data out to your 
disk. This will work if the size of the file is less than the size
of the memory of your computer.</span></p><p><span class="c006">However if this is a large audio or video file, this program may crash
or at least run extremely slowly when your computer runs out of memory.
In order to avoid running out of memory, we retrieve the data in blocks
(or buffers) and then write each block to your disk before retrieving
the next block. This way the program can read any size file without
using up all of the memory you have in your computer.</span></p><pre class="verbatim"><span class="c004">import urllib

img = urllib.urlopen('http://www.py4inf.com/cover.jpg')
fhand = open('cover.jpg', 'w')
size = 0
while True:
    info = img.read(100000)
    if len(info) &lt; 1 : break
    size = size + len(info)
    fhand.write(info)

print size,'characters copied.'
fhand.close()
</span></pre><p><span class="c006">In this example, we read only 100,000 characters at a time and then 
write those characters to the <span class="c001">cover.jpg</span> file
before retrieving the next 100,000 characters of data from the
web.</span></p><p><span class="c006">This program runs as follows:</span></p><pre class="verbatim"><span class="c004">python curl2.py 
568248 characters copied.
</span></pre><p><span class="c006">If you have a Unix or Macintosh computer, you probably have a command
built in to your operating system that performs this operation
as follows:
</span><a id="hevea_default758"></a></p><pre class="verbatim"><span class="c004">curl -O http://www.py4inf.com/cover.jpg
</span></pre><p><span class="c006">The command <span class="c001">curl</span> is short for &#X201C;copy URL&#X201D; and so these two 
examples are cleverly named <span class="c001">curl1.py</span> and <span class="c001">curl2.py</span> on 
<span class="c001">www.py4inf.com/code</span> as they implement similar functionality
to the <span class="c001">curl</span> command. There is also a <span class="c001">curl3.py</span> sample 
program that does this task a little more effectively, in case you
actually want to use this pattern in a program you are writing.</span></p><span class="c005">
</span><h2 class="section" id="sec156"><span class="c006">12.9&#XA0;&#XA0;Glossary</span></h2>
<dl class="description"><dt class="dt-description"><span class="c010">BeautifulSoup:</span></dt><dd class="dd-description"><span class="c006"> A Python library for parsing HTML documents
and extracting data from HTML documents
that compensates for most of the imperfections in the HTML that browsers
generally ignore.
You can download the BeautifulSoup code
from 
<span class="c001">www.crummy.com</span>.
</span><a id="hevea_default759"></a></dd><dt class="dt-description"><span class="c010">port:</span></dt><dd class="dd-description"><span class="c006"> A number that generally indicates which application 
you are contacting when you make a socket connection to a server.
As an example, web traffic usually uses port 80 while email 
traffic uses port 25.
</span><a id="hevea_default760"></a></dd><dt class="dt-description"><span class="c010">scrape:</span></dt><dd class="dd-description"><span class="c006"> When a program pretends to be a web browser and
retrieves a web page, then looks at the web page content. 
Often programs are following the links in one page to find the next
page so they can traverse a network of pages or a social network.
</span><a id="hevea_default761"></a></dd><dt class="dt-description"><span class="c010">socket:</span></dt><dd class="dd-description"><span class="c006"> A network connection between two applications
where the applications can send and receive data in either direction.
</span><a id="hevea_default762"></a></dd><dt class="dt-description"><span class="c010">spider:</span></dt><dd class="dd-description"><span class="c006"> The act of a web search engine retrieving a page and
then all the pages linked from a page and so on until they have 
nearly all of the pages on the Internet which they 
use to build their search index.
</span><a id="hevea_default763"></a></dd></dl><span class="c005">
</span><h2 class="section" id="sec157"><span class="c006">12.10&#XA0;&#XA0;Exercises</span></h2>
<div class="theorem"><span class="c006"><span class="c009">Exercise&#XA0;1</span>&#XA0;&#XA0;<em>
Change the socket program <span class="c001">socket1.py</span> to prompt the user for 
the URL so it can read any web page. 
You can use <span class="c001">split(&#X2019;/&#X2019;)</span> to break the URL into its component parts
so you can extract the host name for the socket <span class="c001">connect</span> call.
Add error checking using <span class="c001">try</span> and <span class="c001">except</span> to handle the condition where the 
user enters an improperly formatted or non-existent URL. 
</em></span></div><div class="theorem"><span class="c006"><span class="c009">Exercise&#XA0;2</span>&#XA0;&#XA0;<em>
Change your socket program so that it counts the number of characters it has received 
and stops displaying any text after it has shown 3000 characters. The program 
should retrieve the entire document and count the total number of characters 
and display the count of the number of characters at the end of the document.
</em></span></div><div class="theorem"><span class="c006"><span class="c009">Exercise&#XA0;3</span>&#XA0;&#XA0;<em>
Use <span class="c001">urllib</span> to replicate the previous exercise of (1) retrieving the document
from a URL, (2) displaying up to 3000 characters, and (3) counting the overall number
of characters in the document. Don&#X2019;t worry about the headers for this exercise, simply
show the first 3000 characters of the document contents.
</em></span></div><div class="theorem"><span class="c006"><span class="c009">Exercise&#XA0;4</span>&#XA0;&#XA0;<em>
Change the <span class="c001">urllinks.py</span> program to extract and count 
paragraph (p) tags from the retrieved HTML document and 
display the count of the paragraphs as the 
output of your program. 
Do not display the paragraph text, only count them.
Test your program on several small web pages
as well as some larger web pages.
</em></span></div><div class="theorem"><span class="c006"><span class="c009">Exercise&#XA0;5</span>&#XA0;&#XA0;<em>
(Advanced) Change the socket program so that it only shows data after the 
headers and a blank line have been received. Remember that <span class="c001">recv</span> is
receiving characters (newlines and all), not lines.
</em></span></div><span class="c005">
</span><hr class="footnoterule" /><dl class="thefootnotes"><dt class="dt-thefootnotes"><span class="c005">
</span><a id="note14" href="#text14"><span class="c006">1</span></a></dt><dd class="dd-thefootnotes"><span class="c006"><div class="footnotetext">The XML format is described in
the next chapter.</div>
</span></dd></dl>
<hr />
<a href="book012.html"><img src="previous_motif.gif" alt="Previous" /></a>
<a href="index.html"><img src="contents_motif.gif" alt="Up" /></a>
<a href="book014.html"><img src="next_motif.gif" alt="Next" /></a>
</body>
</html>
